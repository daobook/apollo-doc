
<!DOCTYPE html>

<html lang="zh_CN">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Perception &#8212; Sphinx documentation</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx13.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css" />
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/tabs.js"></script>
    <script src="../../_static/translations.js"></script>
    <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
    <link rel="canonical" href="https://www.sphinx-doc.org/en/master/docs/specs/perception_apollo_3.0.html" />
    <link rel="search" type="application/opensearchdescription+xml"
          title="在 Sphinx documentation 中搜索"
          href="../../_static/opensearch.xml"/>
    <link rel="shortcut icon" href="../../_static/favicon.svg"/>
    <link rel="index" title="索引" href="../../genindex.html" />
    <link rel="search" title="搜索" href="../../search.html" />
    <link rel="next" title="感知" href="perception_apollo_3.0_cn.html" />
    <link rel="prev" title="Perception" href="perception_apollo_2.5.html" />
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300,400,700'
          rel='stylesheet' type='text/css' />
 
    <style type="text/css">
      table.right { float: right; margin-left: 20px; }
      table.right td { border: 1px solid #ccc; }
      
    </style>
    <script>
      // intelligent scrolling of the sidebar content
      $(window).scroll(function() {
        var sb = $('.sphinxsidebarwrapper');
        var win = $(window);
        var sbh = sb.height();
        var offset = $('.sphinxsidebar').position()['top'];
        var wintop = win.scrollTop();
        var winbot = wintop + win.innerHeight();
        var curtop = sb.position()['top'];
        var curbot = curtop + sbh;
        // does sidebar fit in window?
        if (sbh < win.innerHeight()) {
          // yes: easy case -- always keep at the top
          sb.css('top', $u.min([$u.max([0, wintop - offset - 10]),
                                $(document).height() - sbh - 200]));
        } else {
          // no: only scroll if top/bottom edge of sidebar is at
          // top/bottom edge of window
          if (curtop > wintop && curbot > winbot) {
            sb.css('top', $u.max([wintop - offset - 10, 0]));
          } else if (curtop < wintop && curbot < winbot) {
            sb.css('top', $u.min([winbot - sbh - offset - 20,
                                  $(document).height() - sbh - 200]));
          }
        }
      });
    </script>

  </head><body>
<div class="pageheader">
  <ul>
    <li><a href="../../README.html">Home</a></li>
    <li><a href="../quickstart/README.html">Get it</a></li>
    <li><a href="../../contents.html">Docs</a></li>
    <li><a href="../../development/index.html">Extend</a></li>
  </ul>
  <div>
    <a href="../../README.html">
      <img src="../../_static/sphinx.png" alt="Apollo Doc" />
    </a>
  </div>
</div>

    <div class="related" role="navigation" aria-label="related navigation">
      <h3>导航</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="总目录"
             accesskey="I">索引</a></li>
        <li class="right" >
          <a href="perception_apollo_3.0_cn.html" title="感知"
             accesskey="N">下一页</a> |</li>
        <li class="right" >
          <a href="perception_apollo_2.5.html" title="Perception"
             accesskey="P">上一页</a> |</li>
        <li><a href="../../index.html">Apollo home</a>&#160;|</li>
        <li><a href="../../contents.html">Documentation</a> &#187;</li>

        <li class="nav-item nav-item-this"><a href="">Perception</a></li> 
      </ul>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../../contents.html">目录</a></h3>
  <ul>
<li><a class="reference internal" href="#">Perception</a><ul>
<li><a class="reference internal" href="#introduction">Introduction</a></li>
<li><a class="reference internal" href="#perception-module">Perception module</a><ul>
<li><a class="reference internal" href="#deep-network">Deep Network</a></li>
<li><a class="reference internal" href="#object-detection-tracking">Object Detection/Tracking</a><ul>
<li><a class="reference internal" href="#d-to-3d-bounding-box">2D-to-3D Bounding Box</a></li>
<li><a class="reference internal" href="#object-tracking">Object Tracking</a></li>
</ul>
</li>
<li><a class="reference internal" href="#lane-detection-tracking">Lane Detection/Tracking</a><ul>
<li><a class="reference internal" href="#lane-lines">Lane Lines</a></li>
</ul>
</li>
<li><a class="reference internal" href="#cipv-closest-in-path-vehicle">CIPV (Closest-In Path Vehicle)</a></li>
<li><a class="reference internal" href="#tailgating">Tailgating</a></li>
<li><a class="reference internal" href="#radar-camera-output-fusion">Radar + Camera Output Fusion</a></li>
<li><a class="reference internal" href="#pseudo-lane">Pseudo Lane</a></li>
<li><a class="reference internal" href="#ultrasonic-sensors">Ultrasonic Sensors</a></li>
</ul>
</li>
<li><a class="reference internal" href="#output-of-perception">Output of Perception</a></li>
<li><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>

  <h4>上一个主题</h4>
  <p class="topless"><a href="perception_apollo_2.5.html"
                        title="上一章">Perception</a></p>
  <h4>下一个主题</h4>
  <p class="topless"><a href="perception_apollo_3.0_cn.html"
                        title="下一章">感知</a></p>
  <div role="note" aria-label="source link">
    <h3>本页</h3>
    <ul class="this-page-menu">
      <li><a href="../../_sources/docs/specs/perception_apollo_3.0.md.txt"
            rel="nofollow">显示源代码</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">快速搜索</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="转向" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section class="tex2jax_ignore mathjax_ignore" id="perception">
<h1>Perception<a class="headerlink" href="#perception" title="永久链接至标题">¶</a></h1>
<p>Apollo 3.0
June 27, 2018</p>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="永久链接至标题">¶</a></h2>
<p>Apollo 3.0 introduced a production level solution for the low-cost, closed venue driving scenario that is used as the foundation for commercialized products. The Perception module introduced a few major features to provide more diverse functionalities and a more reliable, robust perception in AV performance, which are:</p>
<ul class="simple">
<li><p><strong>CIPV(Closest In-Path Vehicle) detection and Tailgaiting</strong>: The vehicle in front of the ego-car is detected and its trajectory is estimated for more efficient tailgating and lane keeping when lane detection is unreliable.</p></li>
<li><p><strong>Asynchronous sensor fusion</strong>: unlike the previous version, Perception in Apollo 3.0 is capable of consolidating all the information and data points by asynchronously fusing LiDAR, Radar and Camera data. Such conditions allow for more comprehensive data capture and reflect more practical sensor environments.</p></li>
<li><p><strong>Online pose estimation</strong>: This new feature estimates the pose of an ego-vehicle for every single frame. This feature helps to drive through bumps or slopes on the road with more accurate 3D scene understanding.</p></li>
<li><p><strong>Ultrasonic sensors</strong>: Perception in Apollo 3.0 now works with ultrasonic sensors. The output can be used for Automated Emergency Brake (AEB) and vertical/perpendicular parking.</p></li>
<li><p><strong>Whole lane line</strong>: Unlike previous lane line segments, this whole lane line feature will provide more accurate and long range detection of lane lines.</p></li>
<li><p><strong>Visual localization</strong>: Camera’s are currently being tested to aide and enhance localization</p></li>
<li><p><strong>16 beam LiDAR support</strong></p></li>
</ul>
<p><em><strong>Safety alert</strong></em></p>
<p>Apollo 3.0 <em>does not</em> support a high curvature road, roads without lane lines including local roads and intersections. The perception module is based on visual detection using a deep network with limited data. Therefore, before we release a better network, the driver should be careful while driving and always be ready to disengage the autonomous driving mode by intervening (hit the brakes or turn the steering wheel). While testing Apollo 3.0, please choose a path that has the necessary conditions mentioned above and be vigilant.</p>
<ul class="simple">
<li><p><em><strong>Recommended road</strong></em></p>
<ul>
<li><p><em><strong>Road with clear white lane lines on both sides</strong></em></p></li>
</ul>
</li>
<li><p><em><strong>Avoid</strong></em></p>
<ul>
<li><p><em><strong>High curvature road</strong></em></p></li>
<li><p><em><strong>Road without lane line marks</strong></em></p></li>
<li><p><em><strong>Intersections</strong></em></p></li>
<li><p><em><strong>Dotted lane lines</strong></em></p></li>
<li><p><em><strong>Public roads with a lot of pedestrians or cars</strong></em></p></li>
</ul>
</li>
</ul>
</section>
<section id="perception-module">
<h2>Perception module<a class="headerlink" href="#perception-module" title="永久链接至标题">¶</a></h2>
<p>The flow chart of Apollo 3.0 Perception module:</p>
<p><img alt="Image" src="../../_images/perception_flow_chart_apollo_3.0.png" /></p>
<p>The sub-modules are discussed in the following section.</p>
<section id="deep-network">
<h3>Deep Network<a class="headerlink" href="#deep-network" title="永久链接至标题">¶</a></h3>
<p>Deep Network ingests an image and provides two detection outputs, lane lines and objects for Apollo 3.0. There is an ongoing debate on individual tasks and co-trained tasks for deep learning. Individual networks such as a lane detection network or an object detection network usually perform better than one co-trained multi-task network. However, multiple individual networks will be costly and consume more time in processing. Therefore, the preferred economic choice is co-trained network. In Apollo 3.0, YOLO [1][2] was used as a base network of object and lane segment detection. The object has vehicle, truck, cyclist, and pedestrian categories and represented by a 2-D bounding box with orientation information. The lane lines are detected by segmentation using the same network with some modification. For whole lane line, we have an individual network to provide longer lane lines in cases of either whole or broken lines.</p>
</section>
<section id="object-detection-tracking">
<h3>Object Detection/Tracking<a class="headerlink" href="#object-detection-tracking" title="永久链接至标题">¶</a></h3>
<p>In a traffic setting, there are two kinds of objects: stationary objects and dynamic objects. Stationary objects include lane lines, traffic lights, and thousands of traffic signs written in different languages. Other than driving, there are multiple landmarks on the road mostly for visual localization including street lamps, barriers, bridge on top of the road, or any other skyline construction. Among all those objects, Apollo 3.0 will detect only lane lines.</p>
<p>Among dynamic objects, Apollo can detect passenger vehicles, trucks, cyclists, pedestrians, or any other object including animals on the road. Apollo can also categorize objects based on which lane the object is in. The most important object is CIPV (closest in path vehicle or object). In order of importance, objects present in neighbouring lanes fall in the second category.</p>
<section id="d-to-3d-bounding-box">
<h4>2D-to-3D Bounding Box<a class="headerlink" href="#d-to-3d-bounding-box" title="永久链接至标题">¶</a></h4>
<p>Given a 2D box, with its 3D size and orientation in the camera, this module searches the 3D position in the camera’s coordinate system and estimates an accurate 3D distance using either the width, the height, or the 2D area of that 2D box. The module works without accurate extrinsic camera parameters.</p>
</section>
<section id="object-tracking">
<h4>Object Tracking<a class="headerlink" href="#object-tracking" title="永久链接至标题">¶</a></h4>
<p>The object tracking sub-module utilizes multiple cues such as 3D position, 2D image patches, 2D boxes, or deep learning ROI features. The tracking problem is formulated as multiple hypothesis data association by combining the cues efficiently to provide the most correct association between tracks and detected object, thus obtaining correct ID association for each object.</p>
</section>
</section>
<section id="lane-detection-tracking">
<h3>Lane Detection/Tracking<a class="headerlink" href="#lane-detection-tracking" title="永久链接至标题">¶</a></h3>
<p>Among static objects, we will handle lane lines only, in Apollo 3.0. The lane is for both longitudinal and lateral control. A lane itself guides lateral control and an object in the lane guides longitudinal control.</p>
<section id="lane-lines">
<h4>Lane Lines<a class="headerlink" href="#lane-lines" title="永久链接至标题">¶</a></h4>
<p>We have two types of lane lines, lane mark segment and whole lane line. The lane mark segment is used for visual localization and whole lane line is used for lane keeping.
The lane can be represented by multiple sets of polylines such as next left lane line, left line, right line, and next right lane line. Given a heatmap of lane lines from the Deep Network, the segmented binary image is generated through <strong>Thresholding</strong>. The method first finds the connected components and detects the inner contours. Then it generates lane marker points based on the contour edges in the ground space of ego-vehicle coordinate system. After that, it associates these lane markers into several lane line objects with corresponding relative spatial (e.g., left(L0), right(R0), next left(L1), next right(L2), etc.) labels.</p>
</section>
</section>
<section id="cipv-closest-in-path-vehicle">
<h3>CIPV (Closest-In Path Vehicle)<a class="headerlink" href="#cipv-closest-in-path-vehicle" title="永久链接至标题">¶</a></h3>
<p>A CIPV is the closest vehicle in the ego-lane. An object is represented by 3D bounding box and its 2D projection from the top-down view localizes the object on the ground. Then, each object will be checked if it is in the ego-lane or not. Among the objects in our ego-lane, the closest one will be selected as a CIPV.</p>
</section>
<section id="tailgating">
<h3>Tailgating<a class="headerlink" href="#tailgating" title="永久链接至标题">¶</a></h3>
<p>Tailgating is a maneuver to follow the vehicle or object in front of the autonomous car. From the tracked objects and ego-vehicle motion, the trajectories of objects are estimated. This trajectory will guide how the objects are moving as a group on the road and the future trajectory can be predicted. There is two kinds of tailgating, the one is pure tailgating by following the specific car and the other is CIPV-guided tailgating, which the ego-vehicle follows the CIPV’s trajectory when the no lane line is detected.</p>
<p>The snapshot of visualization of the output is shown in the figure below:
<img alt="Image" src="../../_images/perception_visualization_apollo_3.0.png" /></p>
<p>The figure above depicts visualization of the Perception output in Apollo 3.0. The top left image shows image-based output. The bottom-left image shows the 3D bounding box of objects. Therefore, the left image shows 3-D top-down view of lane lines and objects. The CIPV is marked with a red bounding box. The yellow lines depicts the trajectory of each vehicle</p>
</section>
<section id="radar-camera-output-fusion">
<h3>Radar + Camera Output Fusion<a class="headerlink" href="#radar-camera-output-fusion" title="永久链接至标题">¶</a></h3>
<p>Given multiple sensors, their output should be combined in a synergic fashion. Apollo 3.0. introduces a sensor set with a radar and a camera. For this process, both sensors need to be calibrated. Each sensor will be calibrated using the same method introduced in Apollo 2.0. After calibration, the output will be represented in a 3-D world coordinate system and each output will be fused by their similarity in location, size, time and the utility of each sensor. After learning the utility function of each sensor, the camera contributes more on lateral distance and the radar contributes more on longitudinal distance measurement. Asynchronous sensor fusion algorithm can also be used as an option.</p>
</section>
<section id="pseudo-lane">
<h3>Pseudo Lane<a class="headerlink" href="#pseudo-lane" title="永久链接至标题">¶</a></h3>
<p>All lane detection results will be combined spatially and temporarily to induce the pseudo lane which will be fed to Planning and Control modules. Some lane lines would be incorrect or missing in a certain frame. To provide the smooth lane line output, the history of lane lines using vehicle odometry is used. As the vehicle moves, the odometer of each frame is saved and lane lines in previous frames will be also saved in the history buffer. The detected lane line which does not match with the history lane lines will be removed and the history output will replace the lane line and be provided to the planning module.</p>
</section>
<section id="ultrasonic-sensors">
<h3>Ultrasonic Sensors<a class="headerlink" href="#ultrasonic-sensors" title="永久链接至标题">¶</a></h3>
<p>Apollo 3.0 supports ultrasonic sensors. Each ultrasonic sensor provides the distance of a detected object through the CANBus. The distance measurement from the ultrasonic sensor is then gathered and broadcasted as a ROS topic. In the future, after fusing ultrasonic sensor output, the map of objects and boundary will be published as a ROS output.</p>
</section>
</section>
<section id="output-of-perception">
<h2>Output of Perception<a class="headerlink" href="#output-of-perception" title="永久链接至标题">¶</a></h2>
<p>The input of Planning and Control modules will be quite different with that of the previous Lidar-based system for Apollo 3.0.</p>
<ul class="simple">
<li><p>Lane line output</p>
<ul>
<li><p>Polyline and/or a polynomial curve</p></li>
<li><p>Lane type by position: L1(next left lane line), L0(left lane line), R0(right lane line), R1(next right lane line)</p></li>
</ul>
</li>
<li><p>Object output</p>
<ul>
<li><p>3D rectangular cuboid</p></li>
<li><p>Relative velocity and direction</p></li>
<li><p>Type: CIPV, PIHP, others</p></li>
<li><p>Classification type: car, truck, bike, pedestrian</p></li>
<li><p>Drops: trajectory of an object</p></li>
</ul>
</li>
</ul>
<p>The world coordinate systen is used as ego-coordinate in 3D where the rear center axle is an origin.</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="永久链接至标题">¶</a></h2>
<p>[1] J Redmon, S Divvala, R Girshick, A Farhadi, “You only look once: Unified, real-time object detection,” CVPR 2016</p>
<p>[2] J Redmon, A Farhadi, “YOLO9000: Better, Faster, Stronger,” arXiv preprint</p>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>导航</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="总目录"
             >索引</a></li>
        <li class="right" >
          <a href="perception_apollo_3.0_cn.html" title="感知"
             >下一页</a> |</li>
        <li class="right" >
          <a href="perception_apollo_2.5.html" title="Perception"
             >上一页</a> |</li>
        <li><a href="../../index.html">Apollo home</a>&#160;|</li>
        <li><a href="../../contents.html">Documentation</a> &#187;</li>

        <li class="nav-item nav-item-this"><a href="">Perception</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; 版权所有 2021, xinetzone.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 4.2.0.
    </div>
  </body>
</html>